{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e673d488",
   "metadata": {},
   "source": [
    "1.Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125a766-b096-4282-abf1-d1f6222ebaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Import Necessary Libraries\n",
    "\n",
    "# %%\n",
    "# Á¶ÅÁî®Ë≠¶Âëä\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ê†∏ÂøÉÂ∫ì\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Êú∫Âô®Â≠¶‰π†ÊåáÊ†á\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           precision_recall_curve, roc_curve, auc, \n",
    "                           precision_score, recall_score, f1_score, \n",
    "                           accuracy_score)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import models\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "except ImportError as e:\n",
    "    print(f\"TensorFlow import error: {e}\")\n",
    "\n",
    "# ÂèØËßÜÂåñËÆæÁΩÆ\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7dccd-5d32-4467-b1c8-7463a18a391b",
   "metadata": {},
   "source": [
    " 2. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99440288-d2a2-429f-bac8-aeb65f59e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_data(data_path: str = '../data/augmented') -> Tuple:\n",
    "    \"\"\"Âä†ËΩΩËØÑ‰º∞Êï∞ÊçÆ\"\"\"\n",
    "    print(\"Loading evaluation data...\")\n",
    "    \n",
    "    x_test = np.load(os.path.join(data_path, 'x_test_aug.npy'))\n",
    "    y_test = np.load(os.path.join(data_path, 'y_test_aug.npy'))\n",
    "    \n",
    "    with open(os.path.join(data_path, 'class_names.txt'), 'r', encoding='utf-8') as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    num_classes = len(class_names)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    print(f\"Test data: {x_test.shape} -> {y_test_cat.shape}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    return x_test, y_test, y_test_cat, class_names, num_classes\n",
    "\n",
    "def load_trained_models(results_path: str = '../results') -> Dict[str, Any]:\n",
    "    \"\"\"Âä†ËΩΩËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂíåÁªìÊûú\"\"\"\n",
    "    print(\"Loading trained models and results...\")\n",
    "    \n",
    "    models_info = {}\n",
    "    \n",
    "    try:\n",
    "        # Âä†ËΩΩÊúÄ‰Ω≥Ê®°Âûã‰ø°ÊÅØ\n",
    "        with open(os.path.join(results_path, 'best_model_info.json'), 'r') as f:\n",
    "            best_model_info = json.load(f)\n",
    "        \n",
    "        # Âä†ËΩΩËÆ≠ÁªÉÁªìÊûú\n",
    "        training_results = pd.read_csv(os.path.join(results_path, 'training_results.csv'))\n",
    "        validation_results = pd.read_csv(os.path.join(results_path, 'validation_results.csv'))\n",
    "        \n",
    "        # Âä†ËΩΩËÆ≠ÁªÉÂéÜÂè≤\n",
    "        with open(os.path.join(results_path, 'training_history.json'), 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        \n",
    "        models_info['best_model'] = best_model_info\n",
    "        models_info['training_results'] = training_results\n",
    "        models_info['validation_results'] = validation_results\n",
    "        models_info['training_history'] = training_history\n",
    "        \n",
    "        print(\"Model results loaded successfully!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Results files not found: {e}\")\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "def load_model_predictions(model_path: str, model_name: str, \n",
    "                          x_test: np.ndarray, class_names: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Âä†ËΩΩÊ®°ÂûãÂπ∂ËøõË°åÈ¢ÑÊµã\"\"\"\n",
    "    \n",
    "    model_file = os.path.join(model_path, f\"{model_name}_best.h5\")\n",
    "    \n",
    "    try:\n",
    "        model = models.load_model(model_file)\n",
    "        \n",
    "        # ËøõË°åÈ¢ÑÊµã\n",
    "        y_pred_proba = model.predict(x_test, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "        # ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶\n",
    "        confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "        \n",
    "        predictions = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "        \n",
    "        print(f\"Predictions completed for {model_name}\")\n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "x_test, y_test, y_test_cat, CLASS_NAMES, NUM_CLASSES = load_evaluation_data()\n",
    "\n",
    "# Âä†ËΩΩÊ®°Âûã‰ø°ÊÅØ\n",
    "models_info = load_trained_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40be7256-1b4b-44c8-9142-f404acac0edd",
   "metadata": {},
   "source": [
    "3. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116ec7d-c2b9-48ad-89bc-54cde8cd489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                             y_pred_proba: np.ndarray, class_names: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"ÂÖ®Èù¢ËØÑ‰º∞Ê®°ÂûãÊÄßËÉΩ\"\"\"\n",
    "    \n",
    "    # Âü∫Á°ÄÊåáÊ†á\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # ÂàÜÁ±ªÊä•Âëä\n",
    "    clf_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Ê∑∑Ê∑ÜÁü©Èòµ\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Â§öÁ±ªROCÊõ≤Á∫øÔºàOvRÁ≠ñÁï•Ôºâ\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    \n",
    "    # ËÆ°ÁÆóÊØè‰∏™Á±ªÂà´ÁöÑROCÊõ≤Á∫øÂíåAUC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # ËÆ°ÁÆóÂæÆÂπ≥ÂùáROCÊõ≤Á∫ø\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # ËÆ°ÁÆóÂÆèÂπ≥ÂùáROCÊõ≤Á∫ø\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names))]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(len(class_names)):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= len(class_names)\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'classification_report': clf_report,\n",
    "        'confusion_matrix': cm,\n",
    "        'roc_curves': {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'roc_auc': roc_auc\n",
    "        },\n",
    "        'per_class_metrics': {\n",
    "            'precision': precision_score(y_true, y_pred, average=None),\n",
    "            'recall': recall_score(y_true, y_pred, average=None),\n",
    "            'f1': f1_score(y_true, y_pred, average=None)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42334168-2b05-4060-8899-719a18dc7fc8",
   "metadata": {},
   "source": [
    "4. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a31ba-4d8a-4c17-b9af-2ce7128978d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, class_names: List[str], model_name: str, normalize: bool = True) -> None:\n",
    "    # ÁªòÂà∂Ê∑∑Ê∑ÜÁü©Èòµ\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = f'{model_name} - Normalized Confusion Matrix'\n",
    "    else:\n",
    "        title = f'{model_name} - Confusion Matrix'\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', \n",
    "                cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(roc_data: Dict[str, Any], class_names: List[str], \n",
    "                   model_name: str) -> None:\n",
    "    # ÁªòÂà∂ROCÊõ≤Á∫ø\n",
    "    \n",
    "    fpr = roc_data['fpr']\n",
    "    tpr = roc_data['tpr']\n",
    "    roc_auc = roc_data['roc_auc']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # ÁªòÂà∂ÊØè‰∏™Á±ªÂà´ÁöÑROCÊõ≤Á∫ø\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(class_names)))\n",
    "    for i, color in zip(range(len(class_names)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    # ÁªòÂà∂ÂæÆÂπ≥ÂùáÂíåÂÆèÂπ≥ÂùáROCÊõ≤Á∫ø\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "    \n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n",
    "            color='navy', linestyle=':', linewidth=4)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'{model_name} - ROC Curves', fontsize=16)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(y_true: np.ndarray, y_pred_proba: np.ndarray,\n",
    "                               class_names: List[str], model_name: str) -> None:\n",
    "    # \"\"\"ÁªòÂà∂Á≤æÁ°ÆÁéá-Âè¨ÂõûÁéáÊõ≤Á∫ø\"\"\"\n",
    "    \n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(class_names)))\n",
    "    for i, color in zip(range(len(class_names)), colors):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        ap = auc(recall, precision)\n",
    "        plt.plot(recall, precision, color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AP = {ap:.3f})')\n",
    "    \n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(f'{model_name} - Precision-Recall Curves', fontsize=16)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_per_class_metrics(metrics: Dict[str, np.ndarray], \n",
    "                          class_names: List[str], model_name: str) -> None:\n",
    "   # \"\"\"ÁªòÂà∂ÊØè‰∏™Á±ªÂà´ÁöÑÊÄßËÉΩÊåáÊ†á\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    metrics_data = {\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    }\n",
    "    \n",
    "    for idx, (metric_name, values) in enumerate(metrics_data.items()):\n",
    "        bars = axes[idx].bar(range(len(class_names)), values, color=plt.cm.Set1(range(len(class_names))))\n",
    "        axes[idx].set_title(f'{metric_name} by Class', fontsize=14)\n",
    "        axes[idx].set_xlabel('Class', fontsize=12)\n",
    "        axes[idx].set_ylabel(metric_name, fontsize=12)\n",
    "        axes[idx].set_xticks(range(len(class_names)))\n",
    "        axes[idx].set_xticklabels(class_names, rotation=45)\n",
    "        axes[idx].set_ylim(0, 1.1)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Âú®Êü±Â≠ê‰∏äÊ∑ªÂä†Êï∞ÂÄº\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                          f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Per-Class Performance Metrics', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced0a53-4210-4e1e-9528-acac5606943e",
   "metadata": {},
   "source": [
    "5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def529ff-f33b-435a-96eb-0549c8d18bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                  confidence_scores: np.ndarray, x_test: np.ndarray,\n",
    "                  class_names: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"ÂàÜÊûêÊ®°ÂûãÈîôËØØ\"\"\"\n",
    "    \n",
    "    errors_mask = (y_true != y_pred)\n",
    "    correct_mask = (y_true == y_pred)\n",
    "    \n",
    "    # ÈîôËØØÁªüËÆ°\n",
    "    error_indices = np.where(errors_mask)[0]\n",
    "    correct_indices = np.where(correct_mask)[0]\n",
    "    \n",
    "    error_analysis = {\n",
    "        'total_errors': np.sum(errors_mask),\n",
    "        'error_rate': np.mean(errors_mask),\n",
    "        'error_indices': error_indices,\n",
    "        'correct_indices': correct_indices,\n",
    "        'error_confidences': confidence_scores[errors_mask],\n",
    "        'correct_confidences': confidence_scores[correct_mask],\n",
    "        'misclassification_pairs': []\n",
    "    }\n",
    "    \n",
    "    # ÂàÜÊûêÈîôËØØÁ±ªÂûã\n",
    "    error_pairs = {}\n",
    "    for true_label, pred_label in zip(y_true[errors_mask], y_pred[errors_mask]):\n",
    "        pair = (true_label, pred_label)\n",
    "        error_pairs[pair] = error_pairs.get(pair, 0) + 1\n",
    "    \n",
    "    # ËΩ¨Êç¢‰∏∫ÂèØËØªÊ†ºÂºè\n",
    "    for (true_idx, pred_idx), count in error_pairs.items():\n",
    "        error_analysis['misclassification_pairs'].append({\n",
    "            'true_class': class_names[true_idx],\n",
    "            'predicted_class': class_names[pred_idx],\n",
    "            'count': count,\n",
    "            'percentage': count / len(error_indices) * 100\n",
    "        })\n",
    "    \n",
    "    # ÊåâÈîôËØØÊï∞ÈáèÊéíÂ∫è\n",
    "    error_analysis['misclassification_pairs'].sort(key=lambda x: x['count'], reverse=True)\n",
    "    \n",
    "    return error_analysis\n",
    "\n",
    "def plot_error_analysis(error_analysis: Dict[str, Any], class_names: List[str],\n",
    "                       model_name: str) -> None:\n",
    "    \"\"\"ÁªòÂà∂ÈîôËØØÂàÜÊûêÂõæ\"\"\"\n",
    "    \n",
    "    # ÁΩÆ‰ø°Â∫¶ÂàÜÂ∏ÉÊØîËæÉ\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Ê≠£Á°ÆÂíåÈîôËØØÈ¢ÑÊµãÁöÑÁΩÆ‰ø°Â∫¶ÂàÜÂ∏É\n",
    "    axes[0].hist(error_analysis['correct_confidences'], bins=30, alpha=0.7, \n",
    "                label='Correct Predictions', color='green')\n",
    "    axes[0].hist(error_analysis['error_confidences'], bins=30, alpha=0.7, \n",
    "                label='Wrong Predictions', color='red')\n",
    "    axes[0].set_xlabel('Confidence Score', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Confidence Distribution: Correct vs Wrong', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ÊúÄÂ∏∏ËßÅÁöÑÈîôËØØÁ±ªÂûã\n",
    "    top_errors = error_analysis['misclassification_pairs'][:10]\n",
    "    error_labels = [f\"{err['true_class']}‚Üí{err['predicted_class']}\" for err in top_errors]\n",
    "    error_counts = [err['count'] for err in top_errors]\n",
    "    \n",
    "    axes[1].barh(error_labels, error_counts, color='coral')\n",
    "    axes[1].set_xlabel('Error Count', fontsize=12)\n",
    "    axes[1].set_title('Top 10 Misclassification Patterns', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Error Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ÊâìÂç∞ÈîôËØØÁªüËÆ°\n",
    "    print(f\"Total errors: {error_analysis['total_errors']}\")\n",
    "    print(f\"Error rate: {error_analysis['error_rate']:.3f}\")\n",
    "    print(f\"Average confidence (correct): {np.mean(error_analysis['correct_confidences']):.3f}\")\n",
    "    print(f\"Average confidence (errors): {np.mean(error_analysis['error_confidences']):.3f}\")\n",
    "    \n",
    "    print(\"\\nTop misclassification patterns:\")\n",
    "    for i, error in enumerate(top_errors, 1):\n",
    "        print(f\"{i:2d}. {error['true_class']:>12} ‚Üí {error['predicted_class']:<12} \"\n",
    "              f\"({error['count']:3d} times, {error['percentage']:5.1f}%)\")\n",
    "\n",
    "def visualize_misclassified_examples(x_test: np.ndarray, y_true: np.ndarray,\n",
    "                                   y_pred: np.ndarray, confidence_scores: np.ndarray,\n",
    "                                   class_names: List[str], model_name: str,\n",
    "                                   num_examples: int = 10) -> None:\n",
    "    \"\"\"ÂèØËßÜÂåñÈîôËØØÂàÜÁ±ªÁöÑÁ§∫‰æã\"\"\"\n",
    "    \n",
    "    errors_mask = (y_true != y_pred)\n",
    "    error_indices = np.where(errors_mask)[0]\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"No misclassified examples found!\")\n",
    "        return\n",
    "    \n",
    "    # ÈÄâÊã©‰∏Ä‰∫õÈîôËØØÁ§∫‰æã\n",
    "    selected_indices = error_indices[:min(num_examples, len(error_indices))]\n",
    "    \n",
    "    # ËÆ°ÁÆóÁΩëÊ†ºÂ§ßÂ∞è\n",
    "    n_cols = 5\n",
    "    n_rows = (len(selected_indices) + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 3 * n_rows))\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(x_test[idx].astype('uint8'))\n",
    "        plt.title(f'True: {class_names[y_true[idx]]}\\nPred: {class_names[y_pred[idx]]}\\nConf: {confidence_scores[idx]:.3f}', \n",
    "                 fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Misclassified Examples', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24e38c-18d8-4a2e-8d2f-543ba7e3f426",
   "metadata": {},
   "source": [
    "6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bde2e-4a82-4b0b-adc5-38678ec5100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_performance(models_performance: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"ÊØîËæÉÂ§ö‰∏™Ê®°ÂûãÁöÑÊÄßËÉΩ\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, performance in models_performance.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': performance['accuracy'],\n",
    "            'Precision': performance['precision'],\n",
    "            'Recall': performance['recall'],\n",
    "            'F1-Score': performance['f1_score'],\n",
    "            'Micro AUC': performance['roc_curves']['roc_auc']['micro'],\n",
    "            'Macro AUC': performance['roc_curves']['roc_auc']['macro']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    return df\n",
    "\n",
    "def plot_model_comparison(comparison_df: pd.DataFrame) -> None:\n",
    "    \"\"\"ÁªòÂà∂Ê®°ÂûãÊØîËæÉÂõæ\"\"\"\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Micro AUC', 'Macro AUC']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        bars = axes[i].bar(comparison_df['Model'], comparison_df[metric], \n",
    "                          color=plt.cm.Set1(range(len(comparison_df))))\n",
    "        axes[i].set_title(f'{metric} Comparison', fontsize=14)\n",
    "        axes[i].set_ylabel(metric, fontsize=12)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        axes[i].set_ylim(0, 1.1)\n",
    "        axes[i].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ\n",
    "        for bar, value in zip(bars, comparison_df[metric]):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ÊâìÂç∞ÊØîËæÉË°®Ê†º\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6e129-613f-4444-9ffe-f515646390c0",
   "metadata": {},
   "source": [
    "7. Comprehensive Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f81149-d031-46b6-8d4d-def248add65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_model(model_name: str, model_path: str, \n",
    "                         x_test: np.ndarray, y_test: np.ndarray,\n",
    "                         class_names: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Âçï‰∏™Ê®°ÂûãÁöÑÂÆåÊï¥ËØÑ‰º∞ÊµÅÁ®ã\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Âä†ËΩΩÊ®°ÂûãÈ¢ÑÊµã\n",
    "    predictions = load_model_predictions(model_path, model_name, x_test, class_names)\n",
    "    if predictions is None:\n",
    "        return None\n",
    "    \n",
    "    # ËØÑ‰º∞ÊÄßËÉΩ\n",
    "    performance = evaluate_model_performance(y_test, predictions['y_pred'], \n",
    "                                           predictions['y_pred_proba'], class_names)\n",
    "    \n",
    "    # ÈîôËØØÂàÜÊûê\n",
    "    error_analysis = analyze_errors(y_test, predictions['y_pred'], \n",
    "                                  predictions['confidence_scores'], x_test, class_names)\n",
    "    \n",
    "    # ÂèØËßÜÂåñ\n",
    "    plot_confusion_matrix(performance['confusion_matrix'], class_names, model_name)\n",
    "    plot_roc_curves(performance['roc_curves'], class_names, model_name)\n",
    "    plot_precision_recall_curves(y_test, predictions['y_pred_proba'], class_names, model_name)\n",
    "    plot_per_class_metrics(performance['per_class_metrics'], class_names, model_name)\n",
    "    plot_error_analysis(error_analysis, class_names, model_name)\n",
    "    visualize_misclassified_examples(x_test, y_test, predictions['y_pred'], \n",
    "                                   predictions['confidence_scores'], class_names, model_name)\n",
    "    \n",
    "    # ÊâìÂç∞ËØ¶ÁªÜÊä•Âëä\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(classification_report(y_test, predictions['y_pred'], target_names=class_names))\n",
    "    \n",
    "    # ÂêàÂπ∂ÁªìÊûú\n",
    "    evaluation_results = {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'performance': performance,\n",
    "        'error_analysis': error_analysis\n",
    "    }\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# ËØÑ‰º∞ÊâÄÊúâÂèØÁî®Ê®°Âûã\n",
    "available_models = ['simple_cnn', 'advanced_cnn', 'lightweight_cnn', 'vgg16_transfer']\n",
    "models_evaluation = {}\n",
    "\n",
    "for model_name in available_models:\n",
    "    results = evaluate_single_model(model_name, '../models', x_test, y_test, CLASS_NAMES)\n",
    "    if results is not None:\n",
    "        models_evaluation[model_name] = results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bcaa7-1e3b-46cf-88e7-7860b3f43e4c",
   "metadata": {},
   "source": [
    "8. Final Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151313b2-b41b-4903-a5ad-a223296eca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊØîËæÉÊâÄÊúâÊ®°Âûã\n",
    "if models_evaluation:\n",
    "    performance_comparison = {}\n",
    "    for model_name, evaluation in models_evaluation.items():\n",
    "        performance_comparison[model_name] = evaluation['performance']\n",
    "    \n",
    "    comparison_df = compare_models_performance(performance_comparison)\n",
    "    plot_model_comparison(comparison_df)\n",
    "    \n",
    "    # ÊâæÂà∞ÊúÄ‰Ω≥Ê®°Âûã\n",
    "    best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    print(f\"   BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "    print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "    print(f\"   Macro AUC: {best_model['Macro AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41b429-f373-4650-b817-74fd7a3346a4",
   "metadata": {},
   "source": [
    " 9. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55672b26-1d84-4309-883f-26bc6621f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(models_evaluation: Dict[str, Any], \n",
    "                           comparison_df: pd.DataFrame,\n",
    "                           output_dir: str = '../results') -> None:\n",
    "    \"\"\"‰øùÂ≠òËØÑ‰º∞ÁªìÊûú\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ‰øùÂ≠òÊ®°ÂûãÊØîËæÉÁªìÊûú\n",
    "    comparison_df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
    "    \n",
    "    # ‰øùÂ≠òÊØè‰∏™Ê®°ÂûãÁöÑËØ¶ÁªÜÁªìÊûú\n",
    "    evaluation_summary = {}\n",
    "    for model_name, evaluation in models_evaluation.items():\n",
    "        # Âè™‰øùÂ≠òÂÖ≥ÈîÆÊåáÊ†áÔºåÈÅøÂÖç‰øùÂ≠òÂ§ßÂûãÊï∞ÁªÑ\n",
    "        evaluation_summary[model_name] = {\n",
    "            'accuracy': evaluation['performance']['accuracy'],\n",
    "            'precision': evaluation['performance']['precision'],\n",
    "            'recall': evaluation['performance']['recall'],\n",
    "            'f1_score': evaluation['performance']['f1_score'],\n",
    "            'micro_auc': evaluation['performance']['roc_curves']['roc_auc']['micro'],\n",
    "            'macro_auc': evaluation['performance']['roc_curves']['roc_auc']['macro'],\n",
    "            'error_rate': evaluation['error_analysis']['error_rate'],\n",
    "            'total_errors': evaluation['error_analysis']['total_errors'],\n",
    "            'top_misclassifications': evaluation['error_analysis']['misclassification_pairs'][:5]\n",
    "        }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'evaluation_summary.json'), 'w') as f:\n",
    "        json.dump(evaluation_summary, f, indent=2)\n",
    "    \n",
    "    # ‰øùÂ≠òÂàÜÁ±ªÊä•Âëä\n",
    "    for model_name, evaluation in models_evaluation.items():\n",
    "        clf_report = evaluation['performance']['classification_report']\n",
    "        report_df = pd.DataFrame(clf_report).transpose()\n",
    "        report_df.to_csv(os.path.join(output_dir, f'{model_name}_classification_report.csv'))\n",
    "    \n",
    "    print(f\"Evaluation results saved to {output_dir}\")\n",
    "\n",
    "# ‰øùÂ≠òÁªìÊûú\n",
    "if models_evaluation:\n",
    "    save_evaluation_results(models_evaluation, comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c8612-27d8-4098-aacd-54f2b7f287df",
   "metadata": {},
   "source": [
    "10. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6ac90-5dbc-4022-aeb1-36f0497a9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(models_evaluation: Dict[str, Any],\n",
    "                          comparison_df: pd.DataFrame) -> None:\n",
    "    \"\"\"ÁîüÊàêÊúÄÁªàÊÄªÁªìÊä•Âëä\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FINAL MODEL EVALUATION SUMMARY REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"Number of models evaluated: {len(models_evaluation)}\")\n",
    "    print(f\"Test dataset size: {len(x_test)} samples\")\n",
    "    print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "    \n",
    "    if not comparison_df.empty:\n",
    "        best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "        worst_model = comparison_df.loc[comparison_df['Accuracy'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL: {best_model['Model']}\")\n",
    "        print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "        print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "        print(f\"   Macro AUC: {best_model['Macro AUC']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìä Performance Range:\")\n",
    "        print(f\"   Accuracy: {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}\")\n",
    "        print(f\"   F1-Score: {comparison_df['F1-Score'].min():.4f} - {comparison_df['F1-Score'].max():.4f}\")\n",
    "        \n",
    "        print(f\"\\nüîç Key Insights:\")\n",
    "        for model_name, evaluation in models_evaluation.items():\n",
    "            error_rate = evaluation['error_analysis']['error_rate']\n",
    "            top_error = evaluation['error_analysis']['misclassification_pairs'][0]\n",
    "            print(f\"   {model_name}: Error rate {error_rate:.3f}, \"\n",
    "                  f\"Most common error: {top_error['true_class']}‚Üí{top_error['predicted_class']}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: ../results/\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# ÁîüÊàêÊúÄÁªàÊÄªÁªì\n",
    "generate_final_summary(models_evaluation, comparison_df)\n",
    "\n",
    "print(\"Model evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d29c4-2463-4bca-b988-175582fc4853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91500b-1689-40f2-9df0-b3d05f449ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe487ed-6418-481a-8e8b-581284d0b2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802726d-97b3-4dd0-9f1a-54a5ab2a5b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e40e92-7b13-4f53-93d3-442655ee20ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98487c-d83e-4468-b0dc-f5b7165cdd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67144e9f-bfa2-4cd1-ad4c-8e2caf802a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040b306-df04-4ba2-b674-9f91a64270b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f808ef-0f04-4da6-9f1f-c04fcb7e9ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d53cc9-1b6e-4239-bb87-450b2bf63d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e0642-ff6f-43ee-9927-a43802d9b346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfc61b-6aae-4643-9227-8b8955911de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776e0ae-6eea-4a96-b0c7-f243aa40d89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627e028-0658-43fb-aef2-e784a286f826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c506c-4461-4fe6-87d4-8d946e208acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc07fe-c092-47a8-8fcb-4c90f2d05af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d72d10-6e15-4c71-a50c-977c05f1f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebc2e9-a692-4a8a-ae3f-7e63190f5451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a83856-c387-40a4-a8fc-0085411e9959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59ee64f-ecd1-416e-bde9-ac4423a918e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dec85a-b3e8-4fd1-9a50-8ceaae0be176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db777de9-ac53-4e4e-b82f-daf380d579a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca564c02-115f-4b2a-a4f8-91531944d56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b808b-91cb-41fb-a3b1-de9be4ea7cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f499e44-5045-423d-9901-3ca272e9868c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a77d8-8aeb-4cf3-b8fd-4067df35c68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888afb5-9ae3-452b-96f2-81aa4954e7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89844f7-bc04-4071-aa7b-908278a04a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196d1c8-3895-4232-8c0a-4d0e4bd9aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c95b7-bac3-4d2e-86fe-2add236fbba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91ae84-e4a3-4a61-af2c-14f57df4c4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28898513-dee1-4ad2-a078-692c954f4e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b6196-7fd1-4ac0-bed6-815983ff8be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70221971-5207-460b-8df8-4f10b8bec412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823ceac-38fc-46fa-a83d-0fe27aecfb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65292dab-97df-4975-8fbe-692fbf6be289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-lab1-hku)",
   "language": "python",
   "name": "dl-lab1-hku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
