{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09322c14",
   "metadata": {},
   "source": [
    "1.Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e87a416-14e3-4dec-947f-70b8ba527829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:08:47.101632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758856127.109716   13698 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758856127.112196   13698 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-26 11:08:47.121811: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 核心库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import models, callbacks\n",
    "    from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "except ImportError as e:\n",
    "    print(f\"TensorFlow import error: {e}\")\n",
    "\n",
    "# 设置\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20d5d0-5e60-4f9e-aa49-87c5a29896de",
   "metadata": {},
   "source": [
    "2. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130b1554-522f-4841-a0ec-c7e8063b0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Training data: (40000, 32, 32, 3) -> (40000, 10)\n",
      "Validation data: (10000, 32, 32, 3) -> (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758856130.138692   13698 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded simple_cnn architecture\n",
      "Loaded advanced_cnn architecture\n",
      "Loaded lightweight_cnn architecture\n",
      "Loaded 3 models for training\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(data_path: str = '../data/augmented') -> Tuple:\n",
    "    \"\"\"加载训练数据\"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    \n",
    "    x_train = np.load(os.path.join(data_path, 'x_train_aug.npy'))\n",
    "    y_train = np.load(os.path.join(data_path, 'y_train_aug.npy'))\n",
    "    x_val = np.load(os.path.join(data_path, 'x_val_aug.npy'))\n",
    "    y_val = np.load(os.path.join(data_path, 'y_val_aug.npy'))\n",
    "    \n",
    "    # 转换为分类格式\n",
    "    with open(os.path.join(data_path, 'class_names.txt'), 'r', encoding='utf-8') as f:\n",
    "        class_names = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    num_classes = len(class_names)\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes)\n",
    "    \n",
    "    print(f\"Training data: {x_train.shape} -> {y_train_cat.shape}\")\n",
    "    print(f\"Validation data: {x_val.shape} -> {y_val_cat.shape}\")\n",
    "    \n",
    "    return x_train, y_train_cat, x_val, y_val_cat, class_names, num_classes\n",
    "\n",
    "def load_model_architecture(model_name: str, model_dir: str = '../models') -> models.Model:\n",
    "    \"\"\"加载模型架构\"\"\"\n",
    "    model_path = os.path.join(model_dir, f'{model_name}_config.json')\n",
    "    \n",
    "    with open(model_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    if 'class_name' in config:  # Functional API\n",
    "        model = models.Model.from_config(config)\n",
    "    else:  # Sequential API\n",
    "        model = models.Sequential.from_config(config)\n",
    "    \n",
    "    print(f\"Loaded {model_name} architecture\")\n",
    "    return model\n",
    "\n",
    "# 加载数据\n",
    "x_train, y_train, x_val, y_val, CLASS_NAMES, NUM_CLASSES = load_training_data()\n",
    "\n",
    "# 加载模型\n",
    "MODEL_NAMES = ['simple_cnn', 'advanced_cnn', 'lightweight_cnn']\n",
    "models_dict = {}\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    try:\n",
    "        model = load_model_architecture(model_name)\n",
    "        models_dict[model_name] = model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model {model_name} not found, skipping...\")\n",
    "\n",
    "print(f\"Loaded {len(models_dict)} models for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b2ba1-4b57-43d2-bc2a-8a2486181a86",
   "metadata": {},
   "source": [
    "3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec66bce5-df04-4de3-8c02-447736c05013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 training configurations\n"
     ]
    }
   ],
   "source": [
    "def create_training_configurations() -> List[Dict[str, Any]]:\n",
    "    \"\"\"创建不同的训练配置\"\"\"\n",
    "    \n",
    "    configurations = [\n",
    "        {\n",
    "            'name': 'adam_fast',\n",
    "            'optimizer': 'adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 64,\n",
    "            'epochs': 50,\n",
    "            'patience': 10\n",
    "        },\n",
    "        {\n",
    "            'name': 'adam_slow',\n",
    "            'optimizer': 'adam', \n",
    "            'learning_rate': 0.0001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'patience': 15\n",
    "        },\n",
    "        {\n",
    "            'name': 'sgd_momentum',\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.01,\n",
    "            'batch_size': 64,\n",
    "            'epochs': 80,\n",
    "            'patience': 12\n",
    "        },\n",
    "        {\n",
    "            'name': 'rmsprop',\n",
    "            'optimizer': 'rmsprop',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 60,\n",
    "            'patience': 10\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return configurations\n",
    "\n",
    "def create_callbacks(patience: int = 10, model_name: str = 'model') -> List:\n",
    "    \"\"\"创建训练回调函数\"\"\"\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # 早停\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # 学习率调度\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=patience//2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # 模型检查点\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=f'../models/{model_name}_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        # TensorBoard\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=f'../logs/{model_name}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# 创建训练配置\n",
    "training_configs = create_training_configurations()\n",
    "print(f\"Created {len(training_configs)} training configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d674732-0a79-46c5-a1ab-4c5ced0745d6",
   "metadata": {},
   "source": [
    "4. Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b99196-4367-4c5d-bb02-fa607a83740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train_model(model: models.Model, \n",
    "                          x_train: np.ndarray, y_train: np.ndarray,\n",
    "                          x_val: np.ndarray, y_val: np.ndarray,\n",
    "                          config: Dict[str, Any],\n",
    "                          model_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"编译和训练模型\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} with {config['name']} configuration...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 编译模型\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=config['learning_rate'])\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = SGD(learning_rate=config['learning_rate'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=config['learning_rate'])\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=config['learning_rate'])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # 创建回调\n",
    "    callbacks_list = create_callbacks(config['patience'], \n",
    "                                    f\"{model_name}_{config['name']}\")\n",
    "    \n",
    "    # 训练模型\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # 收集训练结果\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'config_name': config['name'],\n",
    "        'history': history.history,\n",
    "        'training_time': training_time,\n",
    "        'final_epoch': len(history.history['loss']),\n",
    "        'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "        'best_val_loss': min(history.history['val_loss'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Best validation accuracy: {results['best_val_accuracy']:.4f}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f616a-c385-4b30-a21d-b1243468ccd0",
   "metadata": {},
   "source": [
    "5. Training Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059b5b54-b37a-48b0-997e-a45c3ec49c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "\n",
      "Training simple_cnn with adam_fast configuration...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 11:08:52.147397: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n",
      "2025-09-26 11:08:52.607476: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 491520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758856135.351927   13799 service.cc:148] XLA service 0x7f875c010e50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758856135.352051   13799 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-09-26 11:08:55.427215: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1758856135.716319   13799 cuda_dnn.cc:529] Loaded cuDNN version 90101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.1328 - loss: 3.7559   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758856145.277678   13799 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m619/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2723 - loss: 2.2391\n",
      "Epoch 1: val_accuracy improved from None to 0.48210, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.3555 - loss: 1.8435 - val_accuracy: 0.4821 - val_loss: 1.4017 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m620/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5017 - loss: 1.3748\n",
      "Epoch 2: val_accuracy improved from 0.48210 to 0.60960, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.5329 - loss: 1.2987 - val_accuracy: 0.6096 - val_loss: 1.0838 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m624/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6022 - loss: 1.1351\n",
      "Epoch 3: val_accuracy improved from 0.60960 to 0.61010, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6191 - loss: 1.0878 - val_accuracy: 0.6101 - val_loss: 1.0975 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m622/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6567 - loss: 0.9851\n",
      "Epoch 4: val_accuracy improved from 0.61010 to 0.69170, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6667 - loss: 0.9572 - val_accuracy: 0.6917 - val_loss: 0.8772 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m623/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6914 - loss: 0.8858\n",
      "Epoch 5: val_accuracy improved from 0.69170 to 0.69820, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.6991 - loss: 0.8648 - val_accuracy: 0.6982 - val_loss: 0.8690 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7204 - loss: 0.8172\n",
      "Epoch 6: val_accuracy improved from 0.69820 to 0.73320, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.7260 - loss: 0.7994 - val_accuracy: 0.7332 - val_loss: 0.7928 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m623/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.7599\n",
      "Epoch 7: val_accuracy improved from 0.73320 to 0.74780, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.7486 - loss: 0.7426 - val_accuracy: 0.7478 - val_loss: 0.7359 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m622/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7610 - loss: 0.7127\n",
      "Epoch 8: val_accuracy did not improve from 0.74780\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7631 - loss: 0.6993 - val_accuracy: 0.7412 - val_loss: 0.7539 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m621/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7734 - loss: 0.6680\n",
      "Epoch 9: val_accuracy improved from 0.74780 to 0.78150, saving model to ../models/simple_cnn_adam_fast_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7814 - loss: 0.6518 - val_accuracy: 0.7815 - val_loss: 0.6402 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m620/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7884 - loss: 0.6254\n",
      "Epoch 10: val_accuracy did not improve from 0.78150\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7921 - loss: 0.6143 - val_accuracy: 0.7588 - val_loss: 0.7569 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m219/625\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8002 - loss: 0.5984"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting model training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m training_results = \u001b[43mtrain_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_configs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(training_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_all_models\u001b[39m\u001b[34m(models_dict, training_configs)\u001b[39m\n\u001b[32m     13\u001b[39m model_copy.build(model.input_shape)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43mcompile_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     model_results[config[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]] = results\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mcompile_and_train_model\u001b[39m\u001b[34m(model, x_train, y_train, x_val, y_val, config, model_name)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[32m     32\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m training_time = time.time() - start_time\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 收集训练结果\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dl-lab1-hku/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_all_models(models_dict: Dict[str, models.Model],\n",
    "                    training_configs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"训练所有模型\"\"\"\n",
    "    \n",
    "    training_results = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        model_results = {}\n",
    "        \n",
    "        for config in training_configs:\n",
    "            # 创建模型副本用于训练\n",
    "            model_copy = models.clone_model(model)\n",
    "            model_copy.build(model.input_shape)\n",
    "            \n",
    "            try:\n",
    "                results = compile_and_train_model(\n",
    "                    model_copy, x_train, y_train, x_val, y_val, config, model_name\n",
    "                )\n",
    "                model_results[config['name']] = results\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name} with {config['name']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        training_results[model_name] = model_results\n",
    "    \n",
    "    return training_results\n",
    "\n",
    "# 开始训练\n",
    "print(\"Starting model training...\")\n",
    "training_results = train_all_models(models_dict, training_configs)\n",
    "\n",
    "print(f\"\\nTraining completed for {len(training_results)} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7211cf-5f67-481f-a740-e6df079d2461",
   "metadata": {},
   "source": [
    "6. Training Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb90944-f662-4cfb-84fa-fdd7ed938e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: Dict[str, List[float]], \n",
    "                         model_name: str, config_name: str) -> None:\n",
    "    \"\"\"绘制训练历史\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 损失曲线\n",
    "    ax1.plot(history['loss'], label='Training Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title(f'{model_name} - {config_name}\\nLoss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 准确率曲线\n",
    "    ax2.plot(history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title(f'{model_name} - {config_name}\\nAccuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_training_results(training_results: Dict[str, Any]) -> None:\n",
    "    \"\"\"比较训练结果\"\"\"\n",
    "    \n",
    "    results_data = []\n",
    "    \n",
    "    for model_name, model_results in training_results.items():\n",
    "        for config_name, results in model_results.items():\n",
    "            results_data.append({\n",
    "                'Model': model_name,\n",
    "                'Config': config_name,\n",
    "                'Best Val Accuracy': results['best_val_accuracy'],\n",
    "                'Best Val Loss': results['best_val_loss'],\n",
    "                'Training Time (s)': results['training_time'],\n",
    "                'Epochs': results['final_epoch']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # 显示结果表格\n",
    "    print(\"Training Results Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.round(4))\n",
    "    \n",
    "    # 绘制比较图\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(data=df, x='Model', y='Best Val Accuracy', hue='Config')\n",
    "    plt.title('Validation Accuracy by Model and Config')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(data=df, x='Model', y='Training Time (s)', hue='Config')\n",
    "    plt.title('Training Time by Model and Config')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(data=df, x='Model', y='Best Val Loss', hue='Config')\n",
    "    plt.title('Validation Loss by Model and Config')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(data=df, x='Model', y='Epochs', hue='Config')\n",
    "    plt.title('Training Epochs by Model and Config')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 可视化训练结果\n",
    "for model_name, model_results in training_results.items():\n",
    "    for config_name, results in model_results.items():\n",
    "        plot_training_history(results['history'], model_name, config_name)\n",
    "\n",
    "# 比较结果\n",
    "results_df = compare_training_results(training_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89307bb9-f52d-4b35-ad2d-a423c414a58a",
   "metadata": {},
   "source": [
    "7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f269f-f16f-412f-866e-ab6be722cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(model: models.Model,\n",
    "                         x_train: np.ndarray, y_train: np.ndarray,\n",
    "                         x_val: np.ndarray, y_val: np.ndarray,\n",
    "                         param_grid: Dict[str, List]) -> pd.DataFrame:\n",
    "    \"\"\"超参数调优\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for lr in param_grid['learning_rate']:\n",
    "        for bs in param_grid['batch_size']:\n",
    "            for opt in param_grid['optimizer']:\n",
    "                print(f\"\\nTuning: lr={lr}, bs={bs}, opt={opt}\")\n",
    "                \n",
    "                # 创建模型副本\n",
    "                model_copy = models.clone_model(model)\n",
    "                model_copy.build(model.input_shape)\n",
    "                \n",
    "                # 编译\n",
    "                if opt == 'adam':\n",
    "                    optimizer = Adam(learning_rate=lr)\n",
    "                elif opt == 'sgd':\n",
    "                    optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
    "                \n",
    "                model_copy.compile(\n",
    "                    optimizer=optimizer,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "                \n",
    "                # 简短训练进行测试\n",
    "                history = model_copy.fit(\n",
    "                    x_train, y_train,\n",
    "                    batch_size=bs,\n",
    "                    epochs=10,  # 简短的epochs用于调优\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                best_val_acc = max(history.history['val_accuracy'])\n",
    "                best_val_loss = min(history.history['val_loss'])\n",
    "                \n",
    "                results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': bs,\n",
    "                    'optimizer': opt,\n",
    "                    'val_accuracy': best_val_acc,\n",
    "                    'val_loss': best_val_loss\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 超参数网格\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# 选择最佳模型进行调优\n",
    "best_model_name = list(models_dict.keys())[0]\n",
    "best_model = models_dict[best_model_name]\n",
    "\n",
    "print(f\"Performing hyperparameter tuning on {best_model_name}...\")\n",
    "tuning_results = hyperparameter_tuning(best_model, x_train, y_train, x_val, y_val, param_grid)\n",
    "\n",
    "# 显示最佳参数\n",
    "best_params = tuning_results.loc[tuning_results['val_accuracy'].idxmax()]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae79acc-9af1-46c8-b4e9-1330e0198eee",
   "metadata": {},
   "source": [
    "8. Model Evaluation on Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe7d34-09a2-41e5-9750-f82ce502ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models_dict: Dict[str, models.Model],\n",
    "                   training_results: Dict[str, Any],\n",
    "                   x_val: np.ndarray, y_val: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"在验证集上评估模型\"\"\"\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for model_name, model_results in training_results.items():\n",
    "        for config_name, results in model_results.items():\n",
    "            # 加载最佳模型\n",
    "            model_path = f\"../models/{model_name}_{config_name}_best.h5\"\n",
    "            \n",
    "            try:\n",
    "                model = models.load_model(model_path)\n",
    "                \n",
    "                # 评估模型\n",
    "                val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "                \n",
    "                evaluation_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Config': config_name,\n",
    "                    'Validation Loss': val_loss,\n",
    "                    'Validation Accuracy': val_accuracy,\n",
    "                    'Training Time (s)': results['training_time']\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}_{config_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(evaluation_results)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Evaluating models on validation set...\")\n",
    "eval_df = evaluate_models(models_dict, training_results, x_val, y_val)\n",
    "\n",
    "# 显示评估结果\n",
    "print(\"\\nValidation Set Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(eval_df.round(4))\n",
    "\n",
    "# 找到最佳模型\n",
    "best_model_row = eval_df.loc[eval_df['Validation Accuracy'].idxmax()]\n",
    "print(f\"\\nBest Model: {best_model_row['Model']} with {best_model_row['Config']}\")\n",
    "print(f\"Best Validation Accuracy: {best_model_row['Validation Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7ae9e-f475-4656-a4f1-55f29cc205ad",
   "metadata": {},
   "source": [
    "9. Save Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c31c4-de93-420c-bb20-bb4b58d21fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_results(training_results: Dict[str, Any],\n",
    "                         results_df: pd.DataFrame,\n",
    "                         eval_df: pd.DataFrame,\n",
    "                         output_dir: str = '../results') -> None:\n",
    "    \"\"\"保存训练结果\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存训练结果到CSV\n",
    "    results_df.to_csv(os.path.join(output_dir, 'training_results.csv'), index=False)\n",
    "    eval_df.to_csv(os.path.join(output_dir, 'validation_results.csv'), index=False)\n",
    "    \n",
    "    # 保存详细的训练历史\n",
    "    training_history = {}\n",
    "    for model_name, model_results in training_results.items():\n",
    "        training_history[model_name] = {}\n",
    "        for config_name, results in model_results.items():\n",
    "            training_history[model_name][config_name] = {\n",
    "                'history': results['history'],\n",
    "                'training_time': results['training_time'],\n",
    "                'best_metrics': {\n",
    "                    'val_accuracy': results['best_val_accuracy'],\n",
    "                    'val_loss': results['best_val_loss']\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    # 保存最佳模型信息\n",
    "    best_model_info = eval_df.loc[eval_df['Validation Accuracy'].idxmax()].to_dict()\n",
    "    with open(os.path.join(output_dir, 'best_model_info.json'), 'w') as f:\n",
    "        json.dump(best_model_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Training results saved to {output_dir}\")\n",
    "\n",
    "# 保存结果\n",
    "save_training_results(training_results, results_df, eval_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f5d86-a1ba-46a5-8aa9-a7cbfff68067",
   "metadata": {},
   "source": [
    "10. Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3845412-90eb-47ca-a58a-4c2d3432f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_summary(training_results: Dict[str, Any], \n",
    "                            eval_df: pd.DataFrame) -> None:\n",
    "    \"\"\"生成训练总结报告\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL TRAINING SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_models = sum(len(results) for results in training_results.values())\n",
    "    print(f\"Total training runs: {total_models}\")\n",
    "    print(f\"Models trained: {len(training_results)}\")\n",
    "    \n",
    "    # 最佳模型信息\n",
    "    best_model = eval_df.loc[eval_df['Validation Accuracy'].idxmax()]\n",
    "    print(f\"\\nBest Performing Model:\")\n",
    "    print(f\"  Model: {best_model['Model']}\")\n",
    "    print(f\"  Config: {best_model['Config']}\")\n",
    "    print(f\"  Validation Accuracy: {best_model['Validation Accuracy']:.4f}\")\n",
    "    print(f\"  Training Time: {best_model['Training Time (s)']:.2f}s\")\n",
    "    \n",
    "    # 训练统计\n",
    "    avg_accuracy = eval_df['Validation Accuracy'].mean()\n",
    "    max_accuracy = eval_df['Validation Accuracy'].max()\n",
    "    min_accuracy = eval_df['Validation Accuracy'].min()\n",
    "    \n",
    "    print(f\"\\nPerformance Statistics:\")\n",
    "    print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"  Maximum Accuracy: {max_accuracy:.4f}\")\n",
    "    print(f\"  Minimum Accuracy: {min_accuracy:.4f}\")\n",
    "    print(f\"  Accuracy Range: {max_accuracy - min_accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to: ../results/\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# 生成总结\n",
    "generate_training_summary(training_results, eval_df)\n",
    "\n",
    "print(\"Model training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d9f40-9d0e-4e27-87f6-5a05c6657adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9f068-2b3a-49da-95b0-5ed432e4b228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1a268-e00d-44ba-b1ec-c568ae6d57e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ca3f1-2084-4020-91ec-fba347992af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e170c-1515-4760-ad14-da6c7bd0ad25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6637706-a311-484d-8a9a-bcb43392f15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490a760-79d9-46fb-a4e9-4921a9cccedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf7454-55dd-4f1b-8efe-8985d5db5d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c5085-fd1f-4b41-b8d2-50f2237e099f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8cfb4-6e51-420f-8ee9-316bd3b7d84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926630-b85a-4adc-8a3c-4a8964f4c29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9827b-4f06-4e17-8e14-1911469714a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f87ff-3973-4311-9f11-fab7b9996a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056a3c0-0d1b-4695-a062-3e122385297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173cc15-3e1f-4f0d-87e1-62c665ee442c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb583d2-94bf-445d-88c6-9b5afa86765b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d673b3f-ae1a-49dc-bb68-72b2d7daad03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404bee2-cdde-4415-9fc2-2f3415b85671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcc1ce-db06-45ff-acdb-615288ba1c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a712f-2a0a-4f15-b0c9-35b27f485ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d041d-ad44-4adc-b09e-196fab69b569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5cc73-7542-4c32-9dc6-c59bbb022ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1521c9c-2ec9-4818-8afe-2318914bad56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30087623-c0eb-4c69-bb56-5dfa97935043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c0a1f-49cd-4c0c-ad53-aa5eccd3f432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425ba60-6aad-4c1f-98f3-311105553c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f3b92-af87-422e-a0d7-db31ea4fcf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54cdd1-9b8b-4cb7-ae9f-6df1c0b9e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ae810-8639-495f-87c5-a9c4885b9e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd028fc-b682-4a74-9f94-402b0c3a3869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-lab1-hku)",
   "language": "python",
   "name": "dl-lab1-hku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
